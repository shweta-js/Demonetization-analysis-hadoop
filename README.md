
This project aims to analyze the sentiments of Twitter users towards demonetization in India using Hadoop, Pig, and PySpark. Demonetization is a significant economic event that occurred in India in 2016 when the government announced the ban on high-value currency notes of Rs. 500 and Rs. 1000 to curb black money, fake currency, and corruption. Twitter is a popular social media platform where users share their opinions, views, and sentiments about various topics, including demonetization.
To perform the analysis, Twitter data related to demonetization will be collected using Kaggle.. The collected data will include tweets, retweets, replies, and other related information. The collected data will be preprocessed to remove noise, irrelevant information, duplicates, and other anomalies. This will involve text processing techniques such as tokenization, stop-word removal, stemming, and sentiment analysis using natural language processing. PySpark will be used for parallel processing and distributed computing to handle the large volume of data.The preprocessed data will be analyzed using Hadoop, Pig, and PySpark. MapReduce jobs and DataFrame operations will be used to process large datasets and extract relevant information such as user sentiments, the frequency of certain terms, and the overall tone of tweets. The analysis will identify positive, negative, and neutral sentiments towards demonetization and help in understanding the public opinion towards the economic event.


Hadoop MapReduction:
Hadoop MapReduce required the user to write a Map function and a Reduce function in Java to process data. The Map function parses input data and outputs intermediate key-value pairs, while the Reduce function groups the intermediate key-value pairs by key and aggregates them. The MapReduce framework improves the reliability and speed of parallel processing, and it can handle massive scalability of unstructured data stored on thousands of commodity servers. The HDFS architecture and the MapReduce framework run on the same set of nodes because both storage and compute nodes are the same. MapReduce has several benefits, including simplicity, scalability, speed, and fault tolerance. MapReduce can process huge amount of data and parallel processing reduces the time to hours or minutes for jobs that often take days to solve. 
Pig:
Pig is a high-level data flow language that abstracts away many of the details of MapReduce. A Pig script can read data, parse columns, filter out unwanted data, and perform operations such as grouping and counting for the demonetization dataset. The Pig Hadoop framework consists of four main components: Parser, optimizer, compiler, and execution engine. Pig has several advantages, including in-built operators for performing data operations like filter and sort, ease of programming with Pig Latin, automatic optimization of tasks, and the ability to analyse both structured and unstructured data and store the results in HDFS, which helps us to process the dataset quickly by grouping it using tokenization and storing the data.
PySpark:
PySpark processes large datasets in parallel across a cluster of computers. Spark processes data in RAM, while Hadoop MapReduce stores data on the disk. Spark works well with the iterative computations on data that can fit entirely into memory, while Hadoop MapReduce is designed for data that doesnâ€™t fit in memory and can run well alongside other services. Spark is easier to program and has an interactive mode, while Hadoop MapReduce is more difficult to program but has several tools to make it easier. Spark is more cost-effective, while Hadoop MapReduce has more personnel available. Spark's compatibility with data sources is the same as Hadoop MapReduce. Spark can do more than plain data processing, while Hadoop MapReduce is great for batch processing.
